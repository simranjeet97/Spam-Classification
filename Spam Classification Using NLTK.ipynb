{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.classify.util\n",
    "from pandas import DataFrame\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import movie_reviews\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "import io\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NLTK only works with Big Datasets and Provide Good Accuracy for that, If You are using Small Datasets then the Accuracy is Less and Your Model works less accurate.**\n",
    "\n",
    "**But, If You are Using Sklearn library You can work with Both the Small and Big Datasets as well and Get Accuracy above 80%.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "rootdir = 'C:\\\\Users\\\\simra\\\\Desktop\\\\Data Science - Hands On\\\\enron\\\\enron1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simra\\Desktop\\Data Science - Hands On\\enron\\enron1 ['ham', 'spam'] 1\n",
      "C:\\Users\\simra\\Desktop\\Data Science - Hands On\\enron\\enron1\\ham [] 3672\n",
      "C:\\Users\\simra\\Desktop\\Data Science - Hands On\\enron\\enron1\\spam [] 1500\n"
     ]
    }
   ],
   "source": [
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    print(directories,subdirs,len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:\\\\Users\\\\simra\\\\Desktop\\\\Data Science - Hands On\\\\enron', '')\n"
     ]
    }
   ],
   "source": [
    "print(os.path.split(\"C:\\\\Users\\\\simra\\\\Desktop\\\\Data Science - Hands On\\\\enron\\\\\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\simra\\Desktop\\Data Science - Hands On\\enron\\enron1\\ham [] 3672\n",
      "C:\\Users\\simra\\Desktop\\Data Science - Hands On\\enron\\enron1\\spam [] 1500\n"
     ]
    }
   ],
   "source": [
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    if(os.path.split(directories)[1] == 'ham'):\n",
    "        print(directories,subdirs,len(files))\n",
    "        \n",
    "    if(os.path.split(directories)[1] == 'spam'):\n",
    "        print(directories,subdirs,len(files))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_features(words):\n",
    "    my_dict = dict([(word,True) for word in words])\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_list = []\n",
    "spam_list = []\n",
    " \n",
    "\n",
    "for directories, subdirs, files in os.walk(rootdir):\n",
    "    if (os.path.split(directories)[1]  == 'ham'):\n",
    "        for filename in files:      \n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                \n",
    "                words = word_tokenize(data)\n",
    "                \n",
    "                ham_list.append((word_features(words), \"ham\"))\n",
    "    \n",
    "    if (os.path.split(directories)[1]  == 'spam'):\n",
    "        for filename in files:\n",
    "            with open(os.path.join(directories, filename), encoding=\"latin-1\") as f:\n",
    "                data = f.read()\n",
    "                \n",
    "                words = word_tokenize(data)\n",
    "                \n",
    "                spam_list.append((word_features(words), \"spam\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainlist = ham_list + spam_list\n",
    "random.shuffle(mainlist)\n",
    "\n",
    "sets = int(len(mainlist)*.7)\n",
    "train = mainlist[:sets]\n",
    "test = mainlist[sets:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is :  94.71649484536083\n"
     ]
    }
   ],
   "source": [
    "classifier = NaiveBayesClassifier.train(train)\n",
    "accuracy = nltk.classify.util.accuracy(classifier,test)\n",
    "print(\"Accuracy is : \",accuracy*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "               forwarded = True              ham : spam   =    181.4 : 1.0\n",
      "                     hou = True              ham : spam   =    166.5 : 1.0\n",
      "                    2004 = True             spam : ham    =    142.8 : 1.0\n",
      "            prescription = True             spam : ham    =    113.7 : 1.0\n",
      "                    pain = True             spam : ham    =     91.2 : 1.0\n",
      "                    spam = True             spam : ham    =     75.0 : 1.0\n",
      "                     ect = True              ham : spam   =     73.5 : 1.0\n",
      "                     sex = True             spam : ham    =     70.2 : 1.0\n",
      "                  stocks = True             spam : ham    =     70.2 : 1.0\n",
      "                    2001 = True              ham : spam   =     66.8 : 1.0\n",
      "             medications = True             spam : ham    =     65.3 : 1.0\n",
      "              newsletter = True             spam : ham    =     65.3 : 1.0\n",
      "                  weight = True             spam : ham    =     62.1 : 1.0\n",
      "                      ex = True             spam : ham    =     62.1 : 1.0\n",
      "              nomination = True              ham : spam   =     61.3 : 1.0\n",
      "                creative = True             spam : ham    =     58.9 : 1.0\n",
      "                   adobe = True             spam : ham    =     58.9 : 1.0\n",
      "                    2005 = True             spam : ham    =     55.7 : 1.0\n",
      "                  differ = True             spam : ham    =     55.7 : 1.0\n",
      "                     hot = True             spam : ham    =     54.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message is : spam\n"
     ]
    }
   ],
   "source": [
    "words = word_tokenize(\"Free Trip to Goa!! Wow Just in Rupees 1500\")\n",
    "features = word_features(words)\n",
    "print(\"Message is :\" ,classifier.classify(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
